{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Keras Bidirectional LSTM\n",
    "\n",
    "# https://github.com/snehalnair/Named-Entity-Recognition/blob/master/NER.ipynb\n",
    "# https://towardsdatascience.com/named-entity-recognition-ner-using-keras-bidirectional-lstm-28cd3f301f54\n",
    "\n",
    "# https://medium.com/analytics-vidhya/ner-tensorflow-2-2-0-9f10dcf5a0a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./Biomedical_corpora/corpora/NER/CoNLL/BIO/AnatEM_BIO\\\\test.tsv', './Biomedical_corpora/corpora/NER/CoNLL/BIO/AnatEM_BIO\\\\train.tsv', './Biomedical_corpora/corpora/NER/CoNLL/BIO/AnatEM_BIO\\\\valid.tsv']\n"
     ]
    }
   ],
   "source": [
    "sourcepath='./Biomedical_corpora/corpora/NER/CoNLL/BIO/AnatEM_BIO/'\n",
    "tsvpath=sourcepath+'*.tsv'\n",
    "\n",
    "all_files = glob.glob(os.path.join(sourcepath, \"*.tsv\"))    \n",
    "print(all_files)\n",
    "\n",
    "sent_list=[]\n",
    "tag_list=[]\n",
    "split_label=[]\n",
    "split_tags=[]\n",
    "\n",
    "count=0\n",
    "for i in all_files:\n",
    "    with open(i) as tsvfile:\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        \n",
    "        for index,line in enumerate(tsvreader):\n",
    "            if not line:\n",
    "                split_label.append(['sentence'+str(count)]+sent_list)\n",
    "                split_tags.append(['sentence'+str(count)]+tag_list)\n",
    "                sent_list=[]\n",
    "                tag_list=[]\n",
    "                count=count+1\n",
    "                continue\n",
    "#             sent_list.append(line[0])\n",
    "            sent_list.append(line[0])\n",
    "            tag_list.append(line[-1].rstrip(\"\\n\"))\n",
    "                \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe=pd.DataFrame(zip([i[0] for i in split_label],[i[1:] for i in split_label],[i[1:] for i in split_tags]),columns=['Sentence #','Sentence','Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dframe.to_csv('C:/Users/X232747/PycharmProjects/Biomedical_corpora/Biomedical-Corpora-master/corpora/NER/CoNLL/BIO/AnatEM_BIO/AnatEm_sentence_BIO.csv')\n",
    "# dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'B-Anatomical_system', 'I-Anatomical_system', 'B-Cell',\n",
       "       'I-Cell', 'B-Cellular_component', 'B-Organ',\n",
       "       'B-Organism_substance', 'I-Cellular_component', 'B-Cancer',\n",
       "       'I-Cancer', 'B-Developing_anatomical_structure',\n",
       "       'I-Developing_anatomical_structure', 'I-Organism_substance',\n",
       "       'B-Tissue', 'B-Multi-tissue_structure', 'I-Multi-tissue_structure',\n",
       "       'B-Immaterial_anatomical_entity', 'I-Tissue',\n",
       "       'B-Organism_subdivision', 'B-Pathological_formation',\n",
       "       'I-Pathological_formation', 'I-Immaterial_anatomical_entity',\n",
       "       'I-Organ', 'I-Organism_subdivision'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_df=dframe.set_index(['Sentence #']).apply(pd.Series.explode).reset_index()\n",
    "tag_df['Tag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag_df.to_csv('C:/Users/X232747/PycharmProjects/Biomedical_corpora/Biomedical-Corpora-master/corpora/NER/CoNLL/BIO/AnatEM_BIO/AnatEM_BIO_tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-Anatomical_system', 'I-Anatomical_system', 'B-Cell', 'I-Cell', 'B-Cellular_component', 'B-Organ', 'B-Organism_substance', 'I-Cellular_component', 'B-Cancer', 'I-Cancer', 'B-Developing_anatomical_structure', 'I-Developing_anatomical_structure', 'I-Organism_substance', 'B-Tissue', 'B-Multi-tissue_structure', 'I-Multi-tissue_structure', 'B-Immaterial_anatomical_entity', 'I-Tissue', 'B-Organism_subdivision', 'B-Pathological_formation', 'I-Pathological_formation', 'I-Immaterial_anatomical_entity', 'I-Organ', 'I-Organism_subdivision']\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "def get_dict_map(data, token_or_tag):\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        vocab = list(set(data['Sentence'].to_list()))\n",
    "    else:\n",
    "        vocab = list(data['Tag'].unique())\n",
    "        print(vocab)\n",
    "    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n",
    "    return tok2idx, idx2tok\n",
    "\n",
    "\n",
    "token2idx, idx2token = get_dict_map(tag_df, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(tag_df, 'tag')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx\n",
    "tag_df['Word_idx'] = tag_df['Sentence'].map(token2idx)\n",
    "tag_df['Tag_idx'] = tag_df['Tag'].map(tag2idx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>312161</th>\n",
       "      <td>sentence11808</td>\n",
       "      <td>curtail</td>\n",
       "      <td>O</td>\n",
       "      <td>9546</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312162</th>\n",
       "      <td>sentence11808</td>\n",
       "      <td>aberrant</td>\n",
       "      <td>O</td>\n",
       "      <td>14923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312163</th>\n",
       "      <td>sentence11808</td>\n",
       "      <td>EC</td>\n",
       "      <td>B-Cell</td>\n",
       "      <td>8082</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312164</th>\n",
       "      <td>sentence11808</td>\n",
       "      <td>growth</td>\n",
       "      <td>O</td>\n",
       "      <td>3385</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312165</th>\n",
       "      <td>sentence11808</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>18853</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Sentence #  Sentence     Tag  Word_idx  Tag_idx\n",
       "312161  sentence11808   curtail       O      9546        0\n",
       "312162  sentence11808  aberrant       O     14923        0\n",
       "312163  sentence11808        EC  B-Cell      8082        3\n",
       "312164  sentence11808    growth       O      3385        0\n",
       "312165  sentence11808         .       O     18853        0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence #    False\n",
       "Sentence      False\n",
       "Tag           False\n",
       "Word_idx      False\n",
       "Tag_idx       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-db8cc5684c2d>:2: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  tag_df_group = tag_df_fillna.groupby(['Sentence #'],as_index=False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence0</td>\n",
       "      <td>[SUMMARY]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[8426]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence1</td>\n",
       "      <td>[Both, the, innate, and, adaptive, immune, sub...</td>\n",
       "      <td>[O, O, O, O, O, B-Anatomical_system, I-Anatomi...</td>\n",
       "      <td>[5802, 18229, 1835, 6283, 12096, 1191, 14031, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence10</td>\n",
       "      <td>[On, the, 8th, day, of, serum, -, free, cultur...</td>\n",
       "      <td>[O, O, O, O, O, B-Organism_substance, O, O, O,...</td>\n",
       "      <td>[16984, 18229, 9542, 4879, 11455, 17780, 5395,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentence100</td>\n",
       "      <td>[The, pre, -, publication, history, for, this,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[1661, 10799, 5395, 1791, 3313, 1239, 15154, 3...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentence1000</td>\n",
       "      <td>[The, vertebrate, vasculature, develops, in, r...</td>\n",
       "      <td>[O, O, B-Anatomical_system, O, O, O, O, O, O, ...</td>\n",
       "      <td>[1661, 15294, 15226, 17821, 17658, 9712, 8944,...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #                                           Sentence  \\\n",
       "0     sentence0                                          [SUMMARY]   \n",
       "1     sentence1  [Both, the, innate, and, adaptive, immune, sub...   \n",
       "2    sentence10  [On, the, 8th, day, of, serum, -, free, cultur...   \n",
       "3   sentence100  [The, pre, -, publication, history, for, this,...   \n",
       "4  sentence1000  [The, vertebrate, vasculature, develops, in, r...   \n",
       "\n",
       "                                                 Tag  \\\n",
       "0                                                [O]   \n",
       "1  [O, O, O, O, O, B-Anatomical_system, I-Anatomi...   \n",
       "2  [O, O, O, O, O, B-Organism_substance, O, O, O,...   \n",
       "3            [O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4  [O, O, B-Anatomical_system, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                            Word_idx  \\\n",
       "0                                             [8426]   \n",
       "1  [5802, 18229, 1835, 6283, 12096, 1191, 14031, ...   \n",
       "2  [16984, 18229, 9542, 4879, 11455, 17780, 5395,...   \n",
       "3  [1661, 10799, 5395, 1791, 3313, 1239, 15154, 3...   \n",
       "4  [1661, 15294, 15226, 17821, 17658, 9712, 8944,...   \n",
       "\n",
       "                                             Tag_idx  \n",
       "0                                                [0]  \n",
       "1  [0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4               [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_df_fillna = tag_df.fillna(method='ffill', axis=0)\n",
    "tag_df_group = tag_df_fillna.groupby(['Sentence #'],as_index=False\n",
    "                                )['Sentence', 'Tag', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))\n",
    "\n",
    "tag_df_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7921477763730249295\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3044750132\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14572629349709866672\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "assert tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_tokens length: 10628 \n",
      "test_tokens length: 1181 \n",
      "train_tags: 10628 \n",
      "test_tags: 1181\n"
     ]
    }
   ],
   "source": [
    "def get_pad_train_test_val(data_group, data):\n",
    "    n_token = len(list(set(data['Sentence'].to_list())))\n",
    "    n_tag = len(list(set(data['Tag'].to_list())))\n",
    "    \n",
    "    tokens = data_group['Word_idx'].tolist()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)\n",
    "    \n",
    "    tags = data_group['Tag_idx'].tolist()\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx[\"O\"])\n",
    "    \n",
    "    n_tags = len(tag2idx)\n",
    "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
    "    \n",
    "    \n",
    "    train_tokens, test_tokens, train_tags, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n",
    "\n",
    "    print(\n",
    "        'train_tokens length:', len(train_tokens),\n",
    "        '\\ntest_tokens length:', len(test_tokens),\n",
    "        '\\ntrain_tags:', len(train_tags),\n",
    "        '\\ntest_tags:', len(test_tags)\n",
    "    )\n",
    "    \n",
    "    return train_tokens, test_tokens, train_tags, test_tags\n",
    "\n",
    "train_tokens, test_tokens, train_tags, test_tags = get_pad_train_test_val(tag_df_group, tag_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\t[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n"
     ]
    }
   ],
   "source": [
    "for token, tag in zip(train_tokens[0], train_tags[0]):\n",
    "    print('%s\\t%s' % (token, tag))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim:  20162 \n",
      "output_dim:  32 \n",
      "input_length:  334 \n",
      "n_tags:  25\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(list(set(tag_df['Sentence'].to_list())))+1\n",
    "output_dim = 32\n",
    "input_length = max([len(s) for s in tag_df_group['Word_idx'].tolist()])\n",
    "n_tags = len(tag2idx)\n",
    "print('input_dim: ', input_dim, '\\noutput_dim: ', output_dim, '\\ninput_length: ', input_length, '\\nn_tags: ', n_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_lstm_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Embedding layer\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "\n",
    "    # Add bidirectional LSTM\n",
    "    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
    "\n",
    "    # Add LSTM\n",
    "    #     model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
    "\n",
    "    # Add timeDistributed Layer\n",
    "    model.add(TimeDistributed(Dense(n_tags, activation=\"tanh\")))\n",
    "\n",
    "    #Optimiser \n",
    "#     adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 334, 32)           645184    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 334, 64)           16640     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 334, 25)           1625      \n",
      "=================================================================\n",
      "Total params: 663,449\n",
      "Trainable params: 663,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "def train_model(X, y, model):\n",
    "    loss = list()\n",
    "\n",
    "        # fit model for one epoch on this sequence\n",
    "    hist = model.fit(X, y, batch_size=100, verbose=1, epochs=4, validation_split=0.2)\n",
    "    loss.append(hist.history['loss'][0])\n",
    "    return loss\n",
    "model_bilstm_lstm = get_bilstm_lstm_model()\n",
    "plot_model(model_bilstm_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(334, 25)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_tokens)\n",
    "np.shape(train_tags[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "K.set_session(sess)\n",
    "# keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "86/86 [==============================] - 611s 7s/step - loss: 12.1697 - accuracy: 0.8658 - val_loss: 16.0448 - val_accuracy: 0.9941\n",
      "Epoch 2/4\n",
      "86/86 [==============================] - 608s 7s/step - loss: 16.0424 - accuracy: 0.9941 - val_loss: 16.0423 - val_accuracy: 0.9941\n",
      "Epoch 3/4\n",
      "86/86 [==============================] - 620s 7s/step - loss: 16.0404 - accuracy: 0.9941 - val_loss: 16.0407 - val_accuracy: 0.9941\n",
      "Epoch 4/4\n",
      "86/86 [==============================] - 628s 7s/step - loss: 16.0387 - accuracy: 0.9941 - val_loss: 16.0392 - val_accuracy: 0.9941\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>with_add_lstm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.169669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   with_add_lstm\n",
       "0      12.169669"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # !pip install matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(results['with_add_lstm'])\n",
    "plt.xlabel('Epochs');\n",
    "plt.ylabel('Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
