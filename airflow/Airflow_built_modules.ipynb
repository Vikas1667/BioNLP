{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TaskFlow API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIMS_Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nih_grant_etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.utils.timezone import datetime\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'rishabh',\n",
    "    'start_date': datetime(2020, 11, 13)\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id='nih_grant_etl',\n",
    "    default_args=default_args,\n",
    "    schedule_interval=\"00 09 * * 5\",\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "nih_dowload = BashOperator(\n",
    "    task_id='nih_download',\n",
    "    bash_command='cd anaconda3/bin/python download_nih_grant.py',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "nih_extraction = BashOperator(\n",
    "    task_id='nih_data',\n",
    "    bash_command='cd anaconda3/bin/python grant_script.py',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "nih_dowload >> nih_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### publication_etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.utils.timezone import datetime\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'VIkas',\n",
    "    'start_date':datetime(2020, 11, 5)\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id='publication_etl',\n",
    "    default_args=default_args,\n",
    "    schedule_interval='00 20 * * 4'\n",
    "    \n",
    ")\n",
    "\n",
    "publication_download =BashOperator(\n",
    "    task_id='publication_download',\n",
    "    bash_command='cd anaconda3/bin/python download_pubmed_file.py',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "publication_extraction = BashOperator(\n",
    "    task_id='publication_data',\n",
    "    bash_command='cd anaconda3/bin/python xml_to_solr.py',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "publication_download >> publication_extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### patent_etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.utils.timezone import datetime\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'VIkas',\n",
    "    'start_date': datetime(2020, 11, 13)\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id='patent_etl',\n",
    "    default_args=default_args,\n",
    "    schedule_interval='40 16 * * 5',\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "patent_extraction = BashOperator(\n",
    "    task_id='patent_data',\n",
    "    bash_command='cd anaconda3/bin/python uspto_patent_download.py',\n",
    "    dag=dag\n",
    ")\n",
    "        \n",
    "\n",
    "patent_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affiliation_dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.utils.timezone import datetime\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'Vikas',\n",
    "    'start_date': datetime(2021, 3 , 22)\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id='affiliation_dag',\n",
    "    default_args=default_args,\n",
    "    schedule_interval='00 14 * * 5',\n",
    "\n",
    ")\n",
    "\n",
    "patent_extraction = BashOperator(\n",
    "    task_id='affiliation_data',\n",
    "    bash_command='cd /anaconda3/bin/python affiliation_main.py',\n",
    "    dag=dag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aims_stats_daily_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import mysql.connector\n",
    "from datetime import datetime\n",
    "from datetime import datetime as dt\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import datetime\n",
    "import mysql.connector\n",
    "import dateutil.parser as dp\n",
    "\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.utils.timezone import datetime as adt\n",
    "\n",
    "\n",
    "\n",
    "header = {\"Authorization\": \"Basic c29scnJlYWQ6TWVyY2sxMjM0IQ==\"}\n",
    "\n",
    "args = {\n",
    "    'owner': \"Vikas\",\n",
    "    'start_date': adt(2020, 11, 29)\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id='aims_stats_daily_count',\n",
    "    default_args=args,\n",
    "    schedule_interval='00 23 * * 0',\n",
    "    tags=['AIMS']\n",
    ")\n",
    "\n",
    "temp_today_date = datetime.datetime.now().date()\n",
    "\n",
    "def modification_date(filename):\n",
    "    t = os.path.getctime(filename)\n",
    "    return datetime.datetime.fromtimestamp(t)\n",
    "\n",
    "\n",
    "def get_last_seven_date():\n",
    "    today_date = datetime.datetime.now()\n",
    "    #return today_date\n",
    "    ndays = datetime.timedelta(days = 7)\n",
    "    diff_day = today_date - ndays\n",
    "    test_date = diff_day.date()\n",
    "    return test_date\n",
    "\n",
    "def get_last_15_date():\n",
    "    today_date = datetime.datetime.now()\n",
    "    #return today_date\n",
    "    ndays = datetime.timedelta(days = 15)\n",
    "    diff_day = today_date - ndays\n",
    "    test_date = diff_day.date()\n",
    "    return test_date\n",
    "\n",
    "def get_publication_count():\n",
    "    pub_total_count = 0\n",
    "    publication_path = './publication/SOLR_xml/'\n",
    "    for filename in os.listdir(publication_path):\n",
    "        if not filename.endswith('.xml'): continue\n",
    "        fullname = os.path.join(publication_path, filename)\n",
    "        #print(filename)\n",
    "        #tree = ET.parse(fullname)\n",
    "\n",
    "        file_modified_date = modification_date(fullname).date()\n",
    "        if get_last_15_date() <= file_modified_date <= temp_today_date:\n",
    "            print(file_modified_date)\n",
    "            try:\n",
    "                tree = ET.parse(fullname)\n",
    "                pmid_count = tree.findall(\".//field[@name='updated_on']\")\n",
    "                for val in pmid_count:\n",
    "                    convrt_date = dp.parse(val.text).date()\n",
    "                    if get_last_seven_date() <= convrt_date <= temp_today_date:\n",
    "                        pub_total_count+=1\n",
    "            except ET.ParseError:\n",
    "                print(str(filename)+\" solr patent xml file is still updating\")\n",
    "                pub_total_count += 0\n",
    "                pass\n",
    "\n",
    "    return pub_total_count\n",
    "\n",
    "def get_grant_count():\n",
    "    grant_total_count = 0\n",
    "    grant_path = '/mnt4/aims/grant/SOLR_xml'\n",
    "    for filename in os.listdir(grant_path):\n",
    "        if not filename.endswith('.xml'): continue\n",
    "        fullname = os.path.join(grant_path, filename)\n",
    "        #print(filename)\n",
    "        #tree = ET.parse(fullname)\n",
    "\n",
    "        file_modified_date = modification_date(fullname).date()\n",
    "        if get_last_15_date() <= file_modified_date <= temp_today_date:\n",
    "            print(file_modified_date)\n",
    "            try:\n",
    "                tree = ET.parse(fullname)\n",
    "                grant_count = tree.findall(\".//field[@name='updated_on']\")\n",
    "                for val in grant_count:\n",
    "                    convrt_date = dp.parse(val.text).date()\n",
    "                    if get_last_seven_date() <= convrt_date <= temp_today_date:\n",
    "                        grant_total_count+=1\n",
    "            except ET.ParseError:\n",
    "                print(str(filename)+\" solr grant xml file is still updating\")\n",
    "                grant_total_count += 0\n",
    "                pass\n",
    "    return grant_total_count\n",
    "\n",
    "def get_patent_count():\n",
    "    patent_total_count = 0\n",
    "    patent_path = '/mnt4/aims/patent/SOLR_xml'\n",
    "    for filename in os.listdir(patent_path):\n",
    "        if not filename.endswith('.xml'): continue\n",
    "        fullname = os.path.join(patent_path, filename)\n",
    "        #print(filename)\n",
    "        #tree = ET.parse(fullname)\n",
    "\n",
    "        file_modified_date = modification_date(fullname).date()\n",
    "        #print(\"Modified Date is \", file_modified_date.date())\n",
    "\n",
    "        if get_last_15_date() <= file_modified_date <= temp_today_date:\n",
    "            print(file_modified_date)\n",
    "            try:\n",
    "                tree = ET.parse(fullname)\n",
    "                patent_count = tree.findall(\".//field[@name='updated_on']\")\n",
    "                for val in patent_count:\n",
    "                    convrt_date = dp.parse(val.text).date()\n",
    "                    if get_last_seven_date() <= convrt_date <= temp_today_date:\n",
    "                        patent_total_count+=1\n",
    "                #patent_total_count+=patent_count\n",
    "            except ET.ParseError:\n",
    "                print(str(filename)+\" solr patent xml file is still updating\")\n",
    "                patent_total_count += 0\n",
    "                pass\n",
    "    return patent_total_count\n",
    "\n",
    "\n",
    "def get_clinic_count():\n",
    "    clinic_total_count = 0\n",
    "    clinic_path = '/mnt4/aims/clinical_trial/SOLR_xml'\n",
    "    for filename in os.listdir(clinic_path):\n",
    "        if not filename.endswith('.xml'): continue\n",
    "        fullname = os.path.join(clinic_path, filename)\n",
    "        #print(filename)\n",
    "        #tree = ET.parse(fullname)\n",
    "\n",
    "        file_modified_date = modification_date(fullname).date()\n",
    "        #print(\"Modified Date is \", file_modified_date.date())\n",
    "\n",
    "        if get_last_15_date() <= file_modified_date <= temp_today_date:\n",
    "            print(file_modified_date)\n",
    "            try:\n",
    "                tree = ET.parse(fullname)\n",
    "                clinic_count = tree.findall(\".//field[@name='updated_on']\")\n",
    "                for val in clinic_count:\n",
    "                    convrt_date = dp.parse(val.text).date()\n",
    "                    if get_last_seven_date() <= convrt_date <= temp_today_date:\n",
    "                        clinic_total_count+=1\n",
    "            except ET.ParseError:\n",
    "                print(str(filename)+\" solr clinical trial xml file is still updating\")\n",
    "                clinic_total_count += 0\n",
    "                pass\n",
    "    return clinic_total_count\n",
    "\n",
    "\n",
    "\n",
    "def aims_stat_logic_daily():\n",
    "    pubmed_url = \"http://10.121.12.3:8983/solr/pubmed_v1/select?fq=updated_on%3A%5BNOW%2FDAY-7DAY%20TO%20NOW%2FDAY%2B1DAY%5D&q=*%3A*\"\n",
    "    grants_url = \"http://10.121.12.3:8983/solr/grant_v1/select?fq=updated_on%3A%5BNOW%2FDAY-7DAY%20TO%20NOW%2FDAY%2B1DAY%5D&q=*%3A*\"\n",
    "    patent_url = \"http://10.121.12.3:8983/solr/patent_v1/select?fq=updated_on%3A%5BNOW%2FDAY-7DAY%20TO%20NOW%2FDAY%2B1DAY%5D&q=*%3A*\"\n",
    "    ct_url = \"http://10.121.12.3:8983/solr/clinical_trial_v1/select?fq=updated_on%3A%5BNOW%2FDAY-7DAY%20TO%20NOW%2FDAY%2B1DAY%5D&q=*%3A*\"\n",
    "    pub_resp = requests.get(pubmed_url, headers=header)\n",
    "    pub_content = json.loads(pub_resp.content)\n",
    "\n",
    "    grants_resp = requests.get(grants_url, headers=header)\n",
    "    grants_content = json.loads(grants_resp.content)\n",
    "\n",
    "    patent_resp = requests.get(patent_url, headers=header)\n",
    "    patent_content = json.loads(patent_resp.content)\n",
    "\n",
    "    CT_resp = requests.get(ct_url, headers=header)\n",
    "    CT_content = json.loads(CT_resp.content)\n",
    "\n",
    "    count = {}\n",
    "\n",
    "    pub_num_count = pub_content['response']['numFound']\n",
    "    print(pub_num_count)\n",
    "\n",
    "    grant_num_count = grants_content['response']['numFound']\n",
    "    print(grant_num_count)\n",
    "\n",
    "    patent_num_count = patent_content['response']['numFound']\n",
    "    print(patent_num_count)\n",
    "\n",
    "    ct_num_count = CT_content['response']['numFound']\n",
    "    print(ct_num_count)\n",
    "\n",
    "    file_pub_num_count = get_publication_count()\n",
    "    file_grant_num_count = get_grant_count()\n",
    "    file_patent_num_count = get_patent_count()\n",
    "    file_ct_num_count = get_clinic_count()\n",
    "    today = dt.now()\n",
    "\n",
    "    # establishing the connection\n",
    "    conn = mysql.connector.connect(\n",
    "        user='aims', password='9nCBwbQ9!', host='10.121.12.3', database='aims')\n",
    "\n",
    "    # Creating a cursor object using the cursor() method\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Preparing SQL query to INSERT a record into the database.\n",
    "    sql_query = \"\"\"INSERT INTO data_collection_track(\n",
    "       Date, Publications, Patent, Grants, Clinical_trials, Publications_file, Patent_file, Grants_file, Clinical_trials_file)\n",
    "       VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\"\"\"\n",
    "    data_record = (today, pub_num_count, patent_num_count, grant_num_count, ct_num_count, file_pub_num_count, file_patent_num_count, file_grant_num_count,file_ct_num_count)\n",
    "\n",
    "    try:\n",
    "        # Executing the SQL command\n",
    "        cursor.execute(sql_query, data_record)\n",
    "\n",
    "        # Commit your changes in the database\n",
    "        conn.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # Rolling back in case of error\n",
    "        conn.rollback()\n",
    "\n",
    "    # Closing the connection\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "\n",
    "stasts_task_aims_daily_count = PythonOperator(\n",
    "    task_id=\"aims_stats_daily\",\n",
    "    provide_context=False,\n",
    "    python_callable=aims_stat_logic_daily,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# web_hook_task_year = PythonOperator(\n",
    "#     task_id=\"aims_webhook\",\n",
    "#     provide_context=False,\n",
    "#     python_callable=webhook,\n",
    "#     dag=dag\n",
    "# )\n",
    "\n",
    "# stasts_task_aims_daily_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clinical_trial_author_weekly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/mnt4/aims/airflow/dags/author_lib')\n",
    "from name_match import AuthorMatch as au\n",
    "# from author_lib.name_match import AuthorMatch as au\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import uuid\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from xml.sax.saxutils import escape\n",
    "import ast\n",
    "import datetime\n",
    "import traceback\n",
    "\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "from airflow.models import DAG\n",
    "from airflow.utils.timezone import datetime as adt\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "\n",
    "\n",
    "header = {\"Authorization\": \"Basic c29scnJlYWQ6TWVyY2sxMjM0IQ==\"}\n",
    "\n",
    "args = {\n",
    "    'owner': \"Vikas\",\n",
    "    'start_date': days_ago(7)\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id='clinical_trial_author_weekly',\n",
    "    default_args=args,\n",
    "    schedule_interval='00 23 * * 0',\n",
    "    tags=['AIMS']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def get_all_clinic_authors():\n",
    "    clinic_url = 'http://10.121.12.3:8983/solr/clinical_trial_v1/select?fq=updated_on%3A%5BNOW-7DAY%2FDAY%20TO%20NOW%5D&q=central_name%3A*%20AND%20primary_sponsors%3A*&rows=10&start=0'\n",
    "    response = requests.get(clinic_url, headers={'Authorization': \"Basic c29scnJlYWQ6TWVyY2sxMjM0IQ==\"\"\"})\n",
    "    json_data = json.loads(response.text)\n",
    "    new_count = json_data['response']['numFound']\n",
    "    print(new_count)\n",
    "    if new_count==0:\n",
    "        return\n",
    "    new_clinic_url = 'http://10.121.12.3:8983/solr/clinical_trial_v1/select?fq=updated_on%3A%5BNOW-7DAY%2FDAY%20TO%20NOW%5D&q=central_name%3A*%20AND%20primary_sponsors%3A*&rows='+str(20)+'&start=0'\n",
    "    response_2 = requests.get(new_clinic_url, headers={'Authorization': \"Basic c29scnJlYWQ6TWVyY2sxMjM0IQ==\"\"\"})\n",
    "    json_data_2 = json.loads(response_2.text)\n",
    "    title_item_2 = json_data_2['response']['docs']\n",
    "\n",
    "    result_list = []\n",
    "    cnt = 0\n",
    "    for each_item in title_item_2:\n",
    "        try:\n",
    "            cnt += 1\n",
    "            print(cnt)\n",
    "            auth_name_tag = each_item['central_name'][0]\n",
    "            proj_title = each_item['brief_title'][0]\n",
    "            abst_text = each_item['description'][0]\n",
    "            org_text = each_item['primary_sponsors'][0]\n",
    "            tag_text_list = each_item['tags'][0:5]\n",
    "            tag_txt = ''\n",
    "            for tg in tag_text_list:\n",
    "                tag_txt = tag_txt + tg + \",\"\n",
    "\n",
    "            if '$' in auth_name_tag:\n",
    "                auth_name_tag_2 = auth_name_tag.split('$')\n",
    "                for each_name in auth_name_tag_2:\n",
    "                    temp_clinic_dict = {}\n",
    "                    clean_prefix = au.refine_name(each_name.strip())\n",
    "                    if not clean_prefix:\n",
    "                        continue\n",
    "\n",
    "                    check_existing_name = 'http://10.121.12.3:8983/solr/author_v1/select?q=author_name%3A%22'+clean_prefix.title()+'%22'\n",
    "                    response_3 = requests.get(check_existing_name, headers={'Authorization': \"Basic c29scnJlYWQ6TWVyY2sxMjM0IQ==\"\"\"})\n",
    "                    json_data_3 = json.loads(response_3.text)\n",
    "                    new_count_3 = json_data_3['response']['numFound']\n",
    "                    if new_count_3>=1:\n",
    "                        old_auth = 'http://10.121.12.3:8983/solr/author_v1/select?q=author_name%3A%22'+clean_prefix.title()+'%22&wt=json&indent=true'\n",
    "                        response_4 = requests.get(old_auth,\n",
    "                                                  headers={'Authorization': \"Basic c29scnJlYWQ6TWVyY2sxMjM0IQ==\"\"\"})\n",
    "                        json_data_4 = json.loads(response_4.text)\n",
    "                        all_tag = json_data_4['response']['docs'][0]\n",
    "                        temp_clinic_dict['id'] = all_tag['id']\n",
    "                        temp_clinic_dict['original_name'] = clean_prefix.title()\n",
    "                        try:\n",
    "                            if all_tag['clinic_id']:\n",
    "                                all_tag['clinic_id'].append(each_item['id'])\n",
    "                                temp_clinic_dict['clinic_id'] = all_tag['clinic_id']\n",
    "                        except:\n",
    "                            temp_clinic_dict['clinic_id'] = each_item['id']\n",
    "                        try:\n",
    "                            if all_tag['grant_id']:\n",
    "                                temp_clinic_dict['grant_id'] = all_tag['grant_id']\n",
    "                        except:\n",
    "                            pass\n",
    "                        try:\n",
    "                            if all_tag['patent_id']:\n",
    "                                temp_clinic_dict['patent_id'] = all_tag['patent_id']\n",
    "                        except:\n",
    "                            pass\n",
    "                        try:\n",
    "                            if all_tag['PMID']:\n",
    "                                temp_clinic_dict['PMID'] = all_tag['PMID']\n",
    "                        except:\n",
    "                            pass\n",
    "                        all_tag['title'].append(proj_title)\n",
    "                        temp_clinic_dict['Title'] = all_tag['title']\n",
    "                        all_tag['abstract'].append(abst_text)\n",
    "                        temp_clinic_dict['Abstract'] = all_tag['abstract']\n",
    "                        all_tag['tag'].append(tag_txt)\n",
    "                        temp_clinic_dict['tag'] = all_tag['tag']\n",
    "                        temp_clinic_dict['affiliation'] = all_tag['affiliation'][0]\n",
    "                    else:\n",
    "                        temp_clinic_dict['original_name'] = clean_prefix.title()\n",
    "                        temp_clinic_dict['id'] = 'nan'\n",
    "                        temp_clinic_dict['clinic_id'] = each_item['id']\n",
    "                        temp_clinic_dict['Title'] = proj_title\n",
    "                        temp_clinic_dict['Abstract'] = abst_text\n",
    "                        temp_clinic_dict['affiliation'] = org_text\n",
    "                        temp_clinic_dict['tag'] = tag_txt[:-1]\n",
    "                    if temp_clinic_dict != {}:\n",
    "                        result_list.append(temp_clinic_dict)\n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "    print(\"total results count \", len(result_list))\n",
    "    df2 = pd.DataFrame(result_list)\n",
    "    df = df2.drop_duplicates(subset=['original_name', ], keep=False)\n",
    "    empty_str = ''\n",
    "    add_start = '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<add>\\n'\n",
    "    add_end = '</add>\\n'\n",
    "    start_doc = '<doc>\\n'\n",
    "    end_doc = '</doc>\\n'\n",
    "    f_close = '</field>\\n'\n",
    "\n",
    "    empty_str += add_start\n",
    "    for i, row in df.iterrows():\n",
    "        if str(row['clinic_id']) == 'nan':\n",
    "            continue\n",
    "        empty_str += start_doc\n",
    "        if str(row['id']) == 'nan':\n",
    "            empty_str += '<field name=\"id\">' + escape(str(uuid.uuid4())) + f_close\n",
    "        else:\n",
    "            empty_str += '<field name=\"id\">' + escape(str(row['id'])) + f_close\n",
    "        empty_str += '<field name=\"author_name\">' + escape(str(row['original_name'])) + f_close\n",
    "\n",
    "        clinic_count = 0\n",
    "        if type(row['clinic_id'])==list:\n",
    "            for val in row['clinic_id']:\n",
    "                clinic_count+=1\n",
    "                if str(val) != 'nan':\n",
    "                    empty_str += '<field name=\"clinic_id\">' + escape(str(val)) + f_close\n",
    "        else:\n",
    "            empty_str += '<field name=\"clinic_id\">' + escape(str(row['clinic_id'])) + f_close\n",
    "            clinic_count+=1\n",
    "\n",
    "        patent_count = 0\n",
    "        try:\n",
    "            if type(row['patent_id']) == list:\n",
    "                for val in row['patent_id']:\n",
    "                    patent_count+=1\n",
    "                    if str(val) != 'nan':\n",
    "                        empty_str += '<field name=\"patent_id\">' + escape(str(val)) + f_close\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        grant_count = 0\n",
    "        try:\n",
    "            if type(row['grant_id']) == list:\n",
    "                for val in row['grant_id']:\n",
    "                    grant_count += 1\n",
    "                    if str(val) != 'nan':\n",
    "                        empty_str += '<field name=\"grant_id\">' + escape(str(val)) + f_close\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        PMID_count = 0\n",
    "        try:\n",
    "            if type(row['PMID']) == list:\n",
    "                for val in row['PMID']:\n",
    "                    PMID_count += 1\n",
    "                    if str(val) != 'nan':\n",
    "                        empty_str += '<field name=\"PMID\">' + escape(str(val)) + f_close\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "        if type(row['Title']) == list:\n",
    "            for val in row['Title']:\n",
    "                if str(val) != 'nan':\n",
    "                    empty_str += '<field name=\"title\">' + escape(str(val)) + f_close\n",
    "        else:\n",
    "            empty_str += '<field name=\"title\">' + escape(str(row['Title'])) + f_close\n",
    "\n",
    "        if type(row['Abstract']) == list:\n",
    "            for val in row['Abstract']:\n",
    "                if str(val) != 'nan':\n",
    "                    empty_str += '<field name=\"abstract\">' + escape(str(val)) + f_close\n",
    "        else:\n",
    "            empty_str += '<field name=\"abstract\">' + escape(str(row['Abstract'])) + f_close\n",
    "\n",
    "        if type(row['tag']) == list:\n",
    "            for val in row['tag']:\n",
    "                if str(val).lstrip() != 'nan':\n",
    "                    empty_str += '<field name=\"tag\">' + escape(str(val)) + f_close\n",
    "        else:\n",
    "            empty_str += '<field name=\"tag\">' + escape(str(row['tag'])) + f_close\n",
    "\n",
    "        check_type_affi_ori = row['affiliation']\n",
    "        empty_str += '<field name=\"affiliation\">' + escape(str(check_type_affi_ori)) + f_close\n",
    "        empty_str += '<field name=\"total_clinical_trail\">' + str(clinic_count) + f_close\n",
    "        if patent_count!=0:\n",
    "            empty_str += '<field name=\"total_patent\">' + str(patent_count) + f_close\n",
    "        if grant_count!=0:\n",
    "            empty_str += '<field name=\"total_grant\">' + str(grant_count) + f_close\n",
    "        if PMID_count!=0:\n",
    "            empty_str += '<field name=\"total_publication\">' + str(PMID_count) + f_close\n",
    "        empty_str += '<field name=\"total_document_count\">' + str(clinic_count+patent_count+grant_count+PMID_count) + f_close\n",
    "\n",
    "        empty_str += end_doc\n",
    "    empty_str += add_end\n",
    "    time_now  = datetime.datetime.now().strftime('%d_%m_%Y_%H_%M_%S')\n",
    "    file_name = '/mnt4/aims/workflow/people_dag_xml_files/'+str(time_now)+\"_clinic_author.xml\"\n",
    "    with open(file_name, 'a', encoding='utf-8') as xmlFile:\n",
    "        xmlFile.write((empty_str))\n",
    "\n",
    "author_module_regular_update = PythonOperator(\n",
    "    task_id=\"clinical_author_weekly_updater\",\n",
    "    provide_context=False,\n",
    "    python_callable=get_all_clinic_authors,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "author_module_regular_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"response\":{\"numFound\":1392425,\"start\":0,\"docs\":[\n",
    "      {\n",
    "        \"id\":\"7c5659e8-2450-5190-9b71-38d62d80c632\",\n",
    "        \"Company_Name\":[\"ephemera\"],\n",
    "        \"Company_Name_copy_txt\":[\"ephemera\"],\n",
    "        \"Short_Description\":[\"Your true self, unleashed.\"],\n",
    "        \"Description\":[\"Ephemera is a mobile app that allows you to send anonymous geolocalized messages to everyone around you! All the messages are automatically deleted after a few minutes.ephemerapp.com\"],\n",
    "        \"Website\":[\"http://ephemerapp.com\"],\n",
    "        \"Recipient_Country\":[\"Portugal\"],\n",
    "        \"state\":[\"√Årea Metropolitana de Lisboa\"],\n",
    "        \"city\":[\"Lisbon\"],\n",
    "        \"Publisher\":[\"Angel\"],\n",
    "        \"Company_Size\":10,\n",
    "        \"Market\":[\"Social Media\"],\n",
    "        \"Tags\":[\"mobile app\",\n",
    "          \"everyone\",\n",
    "          \"message\",\n",
    "          \"true self\",\n",
    "          \"few minutes.ephemerapp.com\",\n",
    "          \"ephemera\"],\n",
    "        \"_version_\":1688072483867983872\n",
    "      },\n",
    "      {\n",
    "        \"id\":\"586d2d27-8f88-58f5-8194-36a6f74f1a4f\",\n",
    "        \"Company_Name\":[\"Epic Business Consulting\"],\n",
    "        \"Company_Name_copy_txt\":[\"Epic Business Consulting\"],\n",
    "        \"Short_Description\":[\"Analytics For People Who Hate Analytics\"],\n",
    "        \"Description\":[\"Epic gives small to medium sized businesses all the benefits of advanced site and usability analytics with none of the headache. When a company signs up for our service they get a full analytics audit, ensuring that everything is being tracked correctly on their website. We also discuss key performance indicators the business tracks and might suggest a few of our own. Going forward, the company will get bi-weekly status reports in PDF format. The reports will only include KPI's and relevant data, along with targeted alerts and suggestions. An example would be, \\\"65% of Facebook traffic converts into paying customers. You should definitely expand your Facebook marketing efforts.\\\"We continually update the customer on important metrics, and even suggest and implement A/B testing for better conversion. Clients receive full service, stress free analytics without ever seeing a spreadsheet.\"],\n",
    "        \"Website\":[\"http://www.epicbusinessconsulting.com\"],\n",
    "        \"Recipient_Country\":[\"United States\"],\n",
    "        \"state\":[\"Illinois\"],\n",
    "        \"Publisher\":[\"Angel\"],\n",
    "        \"Company_Size\":10,\n",
    "        \"Market\":[\"Small and Medium Businesses\",\n",
    "          \"E-Commerce\",\n",
    "          \"Consulting\",\n",
    "          \"Business Development\"],\n",
    "        \"Team_Member\":[\"{'Epic Business ': \\\"We deliver Epic Business Consulting. What's that you want to hire us? Good Choice. You'll be #excited in no time!\\\"}\"],\n",
    "        \"emerging_term\":[\"stress\",\n",
    "          \"headache\",\n",
    "          \"update\"],\n",
    "        \"Tags\":[\"service\",\n",
    "          \"suggestion\",\n",
    "          \"advanced site\",\n",
    "          \"usability analytics\",\n",
    "          \"company sign\",\n",
    "          \"people\",\n",
    "          \"targeted alert\",\n",
    "          \"client\",\n",
    "          \"headache\",\n",
    "          \"relevant data\",\n",
    "          \"facebook marketing effort\",\n",
    "          \"full analytics audit\",\n",
    "          \"sized business\",\n",
    "          \"customer\",\n",
    "          \"bi-weekly status report\",\n",
    "          \"facebook traffic convert\",\n",
    "          \"company\",\n",
    "          \"example\",\n",
    "          \"pdf format\",\n",
    "          \"full service\",\n",
    "          \"stress free analytics\",\n",
    "          \"better conversion\",\n",
    "          \"analytics\",\n",
    "          \"everything\",\n",
    "          \"website\",\n",
    "          \"important metric\",\n",
    "          \"key performance indicator\",\n",
    "          \"business track\",\n",
    "          \"spreadsheet\",\n",
    "          \"report\",\n",
    "          \"benefit\",\n",
    "          \"implement a/b\"],\n",
    "        \"category_path\":[\"Engineering/Human-machine_interaction/Human-computer_interaction/Usability\"],\n",
    "        \"category_path_copy_txt\":[\"Engineering/Human-machine_interaction/Human-computer_interaction/Usability\"],\n",
    "        \"category\":[\"Engineering\"],\n",
    "        \"category_copy_txt\":[\"Engineering\"],\n",
    "        \"sub_category\":[\"Human-machine_interaction\",\n",
    "          \"Usability\",\n",
    "          \"Human-computer_interaction\"],\n",
    "        \"_version_\":1688072483869032448\n",
    "      },"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## patent_etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import codecs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with io.open(filename) as patent_xml:\n",
    "#     patent_data=patent_xml.read()\n",
    "# print(patent_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "'''from lxml import etree\n",
    "\n",
    "context = etree.iterparse(filename, events=('end',), tag='nodes')\n",
    "\n",
    "for event, element in context:\n",
    "\t<Do the stuff here you want to do with the element. This element has all the information about the content of 'node' tag \n",
    "    and its child elements because in the context above I have ordered it to capture only 'end' events for me. \n",
    "    So it captures the event when the parser hits the end of the node tag i.e </node> tag or <node/> if it has no content inside it.\n",
    "\n",
    "\telement.clear()\n",
    "\t#This line tells that you won't be accessing any child elements of the element now. So the parser can just throw them off.\n",
    "\n",
    "\n",
    "\t#Now clearing the parent elements of the 'element'\n",
    "\twhile elem.getprevious() is not None:\n",
    "    \t\tdel elem.getparent()[0]\n",
    "\t# 'not None' is used here because if the element you are parsing is root itself, then it will raise an exception because there is no parent for it, so you might have to handle that exception too in that case.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml \n",
    "from lxml import etree\n",
    "import xml.etree.cElementTree as ET\n",
    "filename='../Airflow_modules/Patent/uspto_datasets/ipa210520.xml'\n",
    "import re\n",
    "# !pip install xmlformatter\n",
    "# !pip install patentdata\n",
    "# !pip install scispacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename) as f:\n",
    "    patent_xml = f.read()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=re.compile(\"<\\?xml version=\\\"1\\.0\\\" encoding\\=\\\"UTF\\-8\\\"\\?>\")\n",
    "file=text.split(patent_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of patents 8835\n"
     ]
    }
   ],
   "source": [
    "while '' in file:\n",
    "    file.remove('')\n",
    "print('No of patents',len(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patent id ['US20210144899A1']\n",
      "invention_id []\n"
     ]
    }
   ],
   "source": [
    "#patent_id=US20210144906A1-20210520.XML\n",
    "patent_text='<us-patent-application lang=\"EN\" dtd-version=\"v4.4 2014-04-03\" file=\"US20210144899A1-20210520.XML\" status=\"PRODUCTION\" id=\"us-patent-application\" country=\"US\" date-produced=\"20210506\" date-publ=\"20210520\">'\n",
    "patent_id=re.compile('file\\=\\\"([U][S]\\d{11}\\w\\d)\\-\\d{8}\\.XML\\\"')\n",
    "print('patent id',patent_id.findall(patent_text))\n",
    "\n",
    "# inv_title='Agricultural Disc and Process for Manufacturing an Agricultural Disc for Use in Agricultural Work'\n",
    "inv_title='<invention-title id=\"d2e79\">Agricultural Disc and Process for Manufacturing an Agricultural Disc for Use in Agricultural Work</invention-title>'\n",
    "invention_title=re.compile(\"<invention-title id=\\w>\")\n",
    "print('invention_id',invention_title.findall(inv_title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
